{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniil/repos/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py:482: UserWarning: src is not broadcastable to dst, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "  own_state[name].copy_(param)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"/home/daniil/repos/pytorch-segmentation-detection/\")\n",
    "sys.path.insert(0, '/home/daniil/repos/pytorch-segmentation-detection/vision/')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'\n",
    "\n",
    "import torch\n",
    "import pytorch_segmentation_detection.models.resnet_dilated as resnet_dilated\n",
    "from pytorch_segmentation_detection.utils.flops_benchmark import add_flops_counting_methods\n",
    "\n",
    "fcn = resnet_dilated.Resnet18_32s(num_classes=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from conditional_computation import Resnet18_8s_gated, GatedResidualUnit, GatedResidualUnitSpatialSimple\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import pytorch_segmentation_detection.models.resnet_dilated as resnet_dilated\n",
    "from pytorch_segmentation_detection.layers import GlobalAvgPool2d\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from pytorch_segmentation_detection.datasets.pascal_voc import PascalVOCSegmentation\n",
    "\n",
    "import pytorch_segmentation_detection.models.fcn as fcns\n",
    "import pytorch_segmentation_detection.models.resnet_dilated as resnet_dilated\n",
    "from pytorch_segmentation_detection.transforms import (ComposeJoint,\n",
    "                                                       RandomHorizontalFlipJoint,\n",
    "                                                       RandomScaleJoint,\n",
    "                                                       CropOrPad,\n",
    "                                                       ResizeAspectRatioPreserve,\n",
    "                                                       RandomCropJoint,\n",
    "                                                       Split2D)\n",
    "\n",
    "\n",
    "train_transform = ComposeJoint(\n",
    "                [\n",
    "                    RandomHorizontalFlipJoint(),\n",
    "                    RandomCropJoint(crop_size=(224, 224)),\n",
    "                    [transforms.ToTensor(), None],\n",
    "                    [transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), None],\n",
    "                    [None, transforms.Lambda(lambda x: torch.from_numpy(np.asarray(x)).long()) ]\n",
    "                ])\n",
    "\n",
    "\n",
    "trainset = PascalVOCSegmentation('/home/daniil/projects/pascal/dataset/',\n",
    "                                 download=False,\n",
    "                                 joint_transform=train_transform)\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=10,\n",
    "                                          shuffle=True, num_workers=4, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniil/repos/anaconda2/lib/python2.7/site-packages/torch/nn/functional.py:1423: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.upsample instead.\n",
      "  warnings.warn(\"nn.functional.upsample_bilinear is deprecated. Use nn.functional.upsample instead.\")\n"
     ]
    }
   ],
   "source": [
    "fcn = add_flops_counting_methods(fcn)\n",
    "fcn = fcn.cuda().train()\n",
    "\n",
    "fcn.start_flops_count()\n",
    "\n",
    "loader = iter( trainloader )\n",
    "\n",
    "data = loader.next()\n",
    "\n",
    "batch = Variable(data[0].cuda())\n",
    "\n",
    "_ = fcn(batch) # works now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8140887065"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcn.compute_average_flops_cost() / 1e9 / 2 # Result in GFLOPs\n",
    "\n",
    "# All models with 32 output stride should be more-or-less consistent with this page:\n",
    "# https://github.com/albanie/convnet-burden\n",
    "# Which is the case for all resnet models that we have checked."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
